# XAI

The term XAI refers to the movement and efforts made in response to the call for more transparent and interpretable systems. It unites all methods and innovations that aim towards opening the black-box models produced by traditional AI-practices. Its purpose is to allow users to understand, trust and effectively manage the next generation of AI solutions. The majority of XAI techniques that have been developed in recent years, aim to explain supervised machine learning models. To have a better overview over the entirety of all approaches that relate to the concept of XAI it is necessary to broadly categorize methods for explanations into three kinds: Intrinsically Interpretable Methods, Model Agnostic Methods and Example-Based Explanations. The most convenient way to achieve interpretability is to stick with intrinsically interpretable models such as linear regression, logistic regression, and decision trees, hence avoiding the usage of a “black-box” model altogether. Model-agnostic models separate the explanation from the machine learning model, resulting in interchangeable usable XAI – techniques that can be used on all machine learning architectures. The methodology of finding an appropriate explanation for a prediction becomes independent of the model in question. Most notable model agnostic approaches include: 
- [LIME](https://arxiv.org/pdf/1602.04938v1.pdf): where local and interpretable surrogate models are used to approximate individual predictions of the underlying black box model 
- [DeepLift](https://arxiv.org/pdf/1704.02685.pdf): which uses backpropagation through all of the neurons in the network to explain the output. 
- [SHAP](https://github.com/slundberg/shap#citations): which is a unified approach combining techniques from DeepLift and Lime with the concept of shapley values. Shapley Values is a concept borrowed from game theory with the idea that a prediction can be explained by assuming that each feature value of the instance is a ‘’player” in a game where the prediction is the payout. The shapley values aim to calculate the fair distribution of the ‘’payout” among the features, hence explaining the contribution of a specific input or feature to a model’s prediction. 

Example based explanation methods select particular instances of the dataset to explain the behavior of the machine learning model and the distribution of the data in a model agnostic way. In more detail it can be expressed as “A is similar to B and B caused C” so the prediction says A will cause C. Following interpretation methods are all example-based : 
- Counterfactual explanations tell us how an instance must change to significantly change its prediction. By creating counterfactual instances, we learn about how the model makes its predictions and can explain individual predictions. 
- Adversarial examples are counterfactuals used to fool machine learning models. The emphasis is on flipping the prediction and not explaining it. 
- Prototypes are a selection of representative instances from the data and criticisms are instances that are not well represented by those prototypes. 

Another noteworthy subcategory of XAI methods referred to as saliency maps is especially used to explain image classification networks and therefore enabling the end-user to better comprehend the prediction output of the model by creating a visual explanation. These methods aim to highlight the impact of each particular pixel to a prediction. Given the enormous variety of approaches to achieve these pixel attributions, it helps to point out that there exist four main base techniques: 
- Back-propagation methods: these methods spread a feature signal from an output neuron rearward through the layers of the model to the input (e.g. DeConvNet). 
- Gradient-based methods: they compute the gradient of the prediction with respect to the input features, e.g. such as Grad-CAM. They mostly differ in how the gradient is computed. 
- Perturbation-based methods: here, the input is perturbed while keeping track of the overall changes to the output. State-of-the-art RISE algorithm is performing an occlusion of pixels at an intermediate layer of the model 
- Approximation-based methods: these methods involve replacing a deep convolutional neural network (CNN) by a simpler approximation model to easier generate visual explanations. The aforementioned LIME algorithm is an example for an approximation-based method, if used for visual explanation.

## Resources

 - https://arxiv.org/pdf/1602.04938v1.pdf
 - https://github.com/slundberg/shap#citations
 - https://christophm.github.io/interpretable-ml-book/
